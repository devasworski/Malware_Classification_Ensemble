from keras.models import Model
from keras.layers import Input, Dense, Dropout, Flatten, LeakyReLU
from keras.layers.convolutional import Conv1D, MaxPooling1D
from tensorflow.keras.optimizers import Adam
from os import makedirs
from dotenv import load_dotenv
from mlflow import log_artifact, set_experiment, start_run, log_param, log_metrics, end_run
from mlflow.keras import log_model
from mlflow.models.signature import infer_signature
from shutil import rmtree
from utils import Callbacks, Plot
from utils.DataGenerator import BitNet_Generator, get_classweight

Model_Name = 'BitNet.py'
TEMP_FOLDER = '.BitNet'
EPOCHS = 100 #@param {type:"slider", min:0, max:1000, step:5}
LEARNING_RATE = 0.0001 #@param {type:"number"}
BATCH_SIZE = 15 #@param {type:"slider", min:0, max:100, step:5}
DROPOUT = 0.6 #@param {type:"slider", min:0, max:1, step:0.01}
ALPHA = 0.001 #@param {type:"number"}
DATASET = "raw" #@param ["raw", "YongImage", "Upsampled"]

def BitNet_Model():
    """
    Defines the BitNet model with 7 convolutional layers, 4 maxpooling layers, 1 flatten layer, 1 dropout
    layer, 2 fully connected layers and 1 output layer
    :return: The model is being returned.
    """
    input = Input(shape=(18432,1))
    Conv_layer_1 = Conv1D(filters = 16, kernel_size = 8, activation=LeakyReLU(alpha=0.01), name='Conv_layer_1', strides = 8)(input)
    Conv_layer_2 = Conv1D(filters = 16, kernel_size = 3, activation=LeakyReLU(alpha=0.01), name='Conv_layer_2', strides = 1, padding = 'same')(Conv_layer_1)
    maxpoollayer_1 = MaxPooling1D(pool_size=2, name='maxpoollayer_1')(Conv_layer_2)
    Conv_layer_3 = Conv1D(filters = 32, kernel_size = 3, activation=LeakyReLU(alpha=0.01), name='Conv_layer_3', strides = 2, padding = 'same')(maxpoollayer_1)
    maxpoollayer_2 = MaxPooling1D(pool_size=2, name='maxpoollayer_2')(Conv_layer_3)
    Conv_layer_4 = Conv1D(filters = 64, kernel_size = 3, activation=LeakyReLU(alpha=0.01), name='Conv_layer_4', strides = 2, padding = 'same')(maxpoollayer_2)
    maxpoollayer_3 = MaxPooling1D(pool_size=2, name='maxpoollayer_3')(Conv_layer_4)
    Conv_layer_5 = Conv1D(filters = 128, kernel_size = 3, activation=LeakyReLU(alpha=0.01), name='Conv_layer_5', strides = 2, padding= 'same')(maxpoollayer_3)
    maxpoollayer_4 = MaxPooling1D(pool_size=2, name='maxpoollayer_4')(Conv_layer_5)
    Conv_layer_6 = Conv1D(filters = 128, kernel_size = 3, activation=LeakyReLU(alpha=0.01), name='Conv_layer_6', strides = 2, padding= 'same')(maxpoollayer_4)
    Conv_layer_7 = Conv1D(filters = 128, kernel_size = 3, activation=LeakyReLU(alpha=0.01), name='Conv_layer_7', strides = 2, padding= 'same')(Conv_layer_6)
    flatten = Flatten(name='Flatten_Layer')(Conv_layer_7)
    dropout = Dropout(DROPOUT)(flatten)
    FC_layer_1 = Dense(512, activation=LeakyReLU(alpha=0.01), name="FC_layer_1")(dropout)
    output = Dense(9, activation='softmax' , name="output_BitNet")(FC_layer_1)
    model = Model(inputs=input, outputs=output, name="1D_Model")
    optimiser = Adam(learning_rate=LEARNING_RATE)
    model.compile(loss='categorical_crossentropy', optimizer=optimiser, metrics=['accuracy','Recall','Precision'])
    return model

def train(): 
    """
    > The function trains the model, logs the parameters, logs the model, logs the metrics, and plots
    the confusion matrix
    """
    load_dotenv()
    makedirs(TEMP_FOLDER, exist_ok = True)

    class_weight = get_classweight(DATASET)
    train_generator = BitNet_Generator(BATCH_SIZE, 'train', DATASET)
    val_generator = BitNet_Generator(BATCH_SIZE, 'val', DATASET)
    test_generator = BitNet_Generator(BATCH_SIZE, 'test', DATASET)

    set_experiment(Model_Name)
    start_run()
    log_artifact(local_path = 'BitNet.py')
    log_param('dataset',DATASET)
    log_param('dropout',DROPOUT)
    log_param('LeakyReLU alpha',ALPHA)
    log_param('class_weight',class_weight)
    log_param('epochs',EPOCHS)
    log_param('batch_size',BATCH_SIZE)
    log_param('opt_learning_rate',LEARNING_RATE)
    log_param('opt_name','adam')
    log_param('steps_per_epoch',int(train_generator.sample_size() // BATCH_SIZE))
    log_param('validation_steps',int(val_generator.sample_size() // BATCH_SIZE))

    model = BitNet_Model()

    Plot.summary(model, TEMP_FOLDER)

    model.fit(train_generator, epochs = EPOCHS, verbose = 1, validation_data = val_generator, steps_per_epoch = int(train_generator.sample_size()  // BATCH_SIZE),validation_steps = int(val_generator.sample_size() // BATCH_SIZE), callbacks = [Callbacks.CheckpointCallback(TEMP_FOLDER), Callbacks.CustomCallback()], class_weight=class_weight)
    log_artifact(local_path = f'{TEMP_FOLDER}/Checkpoint', artifact_path='')

    model.load_weights(f'{TEMP_FOLDER}/Checkpoint')
    train = train_generator.__getitem__(0)[0]
    predictions = model.predict(train)
    log_model(model,artifact_path='model',signature=infer_signature(train, predictions), registered_model_name=Model_Name)
    score = model.evaluate(test_generator,verbose=1)
    log_metrics({"test_loss": score[0], "test_accuracy": score[1], "test_recall": score[2], "test_precision": score[3]},(EPOCHS-1))
    print(f'Test Loss: {score[0]}, Test Accuracy: {score[1]*100}%, Test Recall: {score[2]*100}%, Test Precision: {score[3]*100}%')

    Plot.plot_confusion_matrix(model,test_generator,  Model_Name, TEMP_FOLDER)
    end_run()
    rmtree(f'{TEMP_FOLDER}/')

if __name__ == "__main__":
    train()