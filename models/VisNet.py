from keras.models import Model
from keras.layers import Input, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from keras.regularizers import l1_l2
from os import makedirs
from dotenv import load_dotenv
from mlflow import log_artifact, set_experiment, start_run, log_param, log_metrics, end_run
from mlflow.keras import log_model
from mlflow.models.signature import infer_signature
from shutil import rmtree
from utils import Callbacks, Plot
from utils.DataGenerator import VisNet_Generator, get_classweight

Model_Name = 'VisNet.py'
TEMP_FOLDER = '.VisNet'
EPOCHS = 400 #@param 
LEARNING_RATE = 0.000007 #@param {type:"number"}
BATCH_SIZE =  200#@param 
DATASET = "raw" #@param ["raw", "YongImage", "Upsampled"]
WEIGHT_DECAY_L1 =  0.0000001#@param
WEIGHT_DECAY_L2 =  0.0000001#@param
DROPOUT = 0.2 #@param

def VisNet_Model():
  input = Input(shape=(1282))
  x = Dense(1282, activation = None, kernel_regularizer=l1_l2(l1 = WEIGHT_DECAY_L1, l2 = WEIGHT_DECAY_L2))(input)
  x = BatchNormalization()(x)
  x = Dropout(0.2)(x)
  x = Dense(1024, activation = None, kernel_regularizer=l1_l2(l1 = WEIGHT_DECAY_L1, l2 = WEIGHT_DECAY_L2))(x)
  x = Dense(1024, activation = None, kernel_regularizer=l1_l2(l1 = WEIGHT_DECAY_L1, l2 = WEIGHT_DECAY_L2))(x)
  x = BatchNormalization()(x)
  x = Dropout(0.2)(x)
  x = Dense(1024, activation = None, kernel_regularizer=l1_l2(l1 = WEIGHT_DECAY_L1, l2 = WEIGHT_DECAY_L2))(x)
  x = Dense(1024, activation = None, kernel_regularizer=l1_l2(l1 = WEIGHT_DECAY_L1, l2 = WEIGHT_DECAY_L2))(x)
  x = BatchNormalization()(x)
  x = Dropout(0.2)(x)
  x = Dense(1024, activation = None, kernel_regularizer=l1_l2(l1 = WEIGHT_DECAY_L1, l2 = WEIGHT_DECAY_L2))(x)
  x = BatchNormalization()(x)
  x = Dropout(0.2)(x)
  output = Dense(9, activation='softmax' , name="output_VisNet")(x)
  model = Model(inputs=input, outputs=output, name="Entropy_Model")
  model.compile(loss='categorical_crossentropy', optimizer = Adam(learning_rate = LEARNING_RATE), metrics=['accuracy','Recall','Precision'], run_eagerly=True)
  return model

def train():
    load_dotenv()
    makedirs(TEMP_FOLDER, exist_ok = True)
    class_weight = get_classweight(DATASET)
    train_generator = VisNet_Generator(BATCH_SIZE, 'train', DATASET)
    val_generator = VisNet_Generator(BATCH_SIZE, 'val', DATASET)
    test_generator = VisNet_Generator(BATCH_SIZE, 'test', DATASET)

    set_experiment(Model_Name)
    start_run()
    log_artifact(local_path = 'VisNet.py')
    log_param('dataset',DATASET)
    log_param('Eager Execution',True)
    log_param('class_weight',class_weight)
    log_param('epochs',EPOCHS)
    log_param('batch_size',BATCH_SIZE)
    log_param('opt_learning_rate',LEARNING_RATE)
    log_param('opt_name','Adam')
    log_param('steps_per_epoch',int(train_generator.sample_size() // BATCH_SIZE))
    log_param('validation_steps',int(val_generator.sample_size() // BATCH_SIZE))
    log_param('weight_decay_L1',WEIGHT_DECAY_L1)
    log_param('weight_decay_L2',WEIGHT_DECAY_L2)
    log_param('dropout',DROPOUT)
    
    model = VisNet_Model()
    Plot.summary(model, TEMP_FOLDER)

    model.fit(train_generator, epochs = EPOCHS, verbose = 1, validation_data = val_generator, steps_per_epoch = int(train_generator.sample_size()  // BATCH_SIZE),validation_steps = int(val_generator.sample_size() // BATCH_SIZE), callbacks = [Callbacks.CheckpointCallback(TEMP_FOLDER), Callbacks.CustomCallback()], class_weight=class_weight)
    log_artifact(local_path = f'{TEMP_FOLDER}/Checkpoint', artifact_path='')

    model.load_weights(f'{TEMP_FOLDER}/Checkpoint')
    train = train_generator.__getitem__(0)[0]
    predictions = model.predict(train)
    log_model(model,artifact_path='model',signature=infer_signature(train, predictions), registered_model_name=Model_Name)
    score = model.evaluate(test_generator,verbose=1)
    log_metrics({"test_loss": score[0], "test_accuracy": score[1], "test_recall": score[2], "test_precision": score[3]},(EPOCHS-1))
    print(f'Test Loss: {score[0]}, Test Accuracy: {score[1]*100}%, Test Recall: {score[2]*100}%, Test Precision: {score[3]*100}%')

    Plot.plot_confusion_matrix(model, test_generator, Model_Name, TEMP_FOLDER)
    end_run()
    rmtree(f'{TEMP_FOLDER}/')

if __name__ == "__main__":
    train()