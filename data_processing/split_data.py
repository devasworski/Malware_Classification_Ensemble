# This files prepares the splits of the dataset for model training.

import os
from tensorflow.keras.utils import to_categorical
import csv
import numpy as np
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
import pandas as pd
from sklearn.utils import resample

if not os.path.exists('../data/splits/'):
  os.makedirs("../data/splits/")


## Raw Dataset
'''
As the dataset is to big to be loaded into most environments, a DataGenerator structure has bee Build in Keras. This will load the dataset in batched. For this, three files have been created. Each file respectively specifies which samples belong to test, validation or train. The test samples will be taken from the train dataset, as the test dataset has no true lables.
In this section, the train, val & test splitt will remain as it is. Not data augumentation other then the splitt will be performed.
'''
if not os.path.exists("../data/splits/raw/"):
  os.makedirs('../data/splits/raw')

  samples = {}
  with open('../data/BIG_2015/trainLabels.csv', mode='r') as infile:
    reader = csv.reader(infile)
    label_dict = dict((rows[0],rows[1]) for rows in reader)

  for subdir, dirs, files in os.walk('../data/splits/train'):
    for file in files:
      sample_id = file.replace(".asm", "").replace(".bytes", "")
      sample_lable = int(label_dict.get(sample_id,'0'))-1
      samples.update({sample_id:sample_lable})
  ids = []
  lables = []
  for id, lable in samples.items():
    ids.append(id)
    lables.append(lable)
  lables = to_categorical(lables, num_classes=9)
  np.save('../data/splits/raw/all_train_filenames.npy', ids)
  np.save('../data/splits/raw/all_train_labels.npy', lables)

  ids_shuffled, labels_shuffled = shuffle(ids, lables)
  np.save('../data/splits/raw/all_shuffled_train_filenames.npy', ids_shuffled)
  np.save('../data/splits/pure/all_shuffled_train_labels.npy', labels_shuffled)

  # as there are no lables for the test dataset, the train dataset will be used for creating the testing dataset
  x_train, x_test, y_train, y_test = train_test_split(np.array(ids_shuffled), labels_shuffled, test_size=0.1, random_state=False, stratify=labels_shuffled)
  x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=False, stratify=y_train)
    
  np.save('../data/splits/raw/x_train.npy', x_train)  
  np.save('../data/splits/raw/y_train.npy', y_train)  
  np.save('../data/splits/raw/x_test.npy', x_test)  
  np.save('../data/splits/raw/y_test.npy', y_test)  
  np.save('../data/splits/raw/x_val.npy', x_val)  
  np.save('../data/splits/raw/y_val.npy', y_val)
  print('Raw Dataset split created')
else: 
  print('Raw Dataset split already exists')


## YongImage Dataset
'''
As the dataset is to big to be loaded into most environments, a DataGenerator structure has bee Build in Keras. This will load the dataset in batched. For this, three files have been created. Each file respectively specifies which samples belong to test, validation or train. The test samples will be taken from the train dataset, as the test dataset has no true lables.
In this section, the train, val & test splitt will be prepared accordingly to the YongImage paper. **[1]** For this the classes F4, F5 & F7 will be randomly upsamples to 500. This causes data leakage, which has not been considered within the original paper.

**[1]** Y. Jiang, S. Li, Y. Wu, and F. Zou, ‘A Novel Image-Based Malware Classification Model Using Deep Learning’, in Neural Information Processing, vol. 11954, T. Gedeon, K. W. Wong, and M. Lee, Eds. Cham: Springer International Pub, 2019, pp. 150–161. doi: 10.1007/978-3-030-36711-4_14.
'''

if not os.path.exists("../data/splits/YongImage/"):
  os.makedirs('../data/splits/YongImage')

  df = pd.DataFrame(np.load('../data/splits/raw/all_shuffled_train_filenames.npy'),columns=['id'])
  df['lables'] = pd.DataFrame(np.argmax(np.load('../data/splits/raw/all_shuffled_train_labels.npy'),axis=1))
  F1, F2, F3, F4, F5, F6, F7, F8, F9 = [x for _, x in df.groupby(df['lables'])]
  F4 = resample(F4, replace=True, n_samples=500, random_state=42)
  F5 = resample(F5, replace=True, n_samples=500, random_state=42)
  F7 = resample(F7, replace=True, n_samples=500, random_state=42)
  df = pd.concat([F1, F2, F3, F4, F5, F6, F7, F8, F9])
  ids = df['id'].to_numpy()
  lables = df['lables'].to_numpy()
  lables = to_categorical(lables, num_classes=9)

  # as there are no lables for the test dataset, the train dataset will be used for creating the testing dataset
  x_train, x_test, y_train, y_test = train_test_split(np.array(ids), lables, test_size=0.1, random_state=False, stratify=lables)
  x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=False, stratify=y_train)

  np.save('../data/splits/YongImage/x_train.npy', x_train)  
  np.save('../data/splits/YongImage/y_train.npy', y_train)  
  np.save('../data/splits/YongImage/x_test.npy', x_test)  
  np.save('../data/splits/YongImage/y_test.npy', y_test)  
  np.save('../data/splits/YongImage/x_val.npy', x_val)  
  np.save('../data/splits/YongImage/y_val.npy', y_val)
  print('YongImage Dataset split created')
else: 
  print('YongImage Dataset split already exists')


## Upsampled
'''
As the dataset is to big to be loaded into most environments, a DataGenerator structure has bee Build in Keras. This will load the dataset in batched. For this, three files have been created. Each file respectively specifies which samples belong to test, validation or train. The test samples will be taken from the train dataset, as the test dataset has no true lables.
In this section, the train, val & test splitt will be prepared similar to the YongImage paper. **[1]** For this the classes F4, F5 & F7 will be randomly upsamples. In order to prevent data leaking, which has not been considered within the original paper, only the Train data will be upsampled to 400. 

**[1]** Y. Jiang, S. Li, Y. Wu, and F. Zou, ‘A Novel Image-Based Malware Classification Model Using Deep Learning’, in Neural Information Processing, vol. 11954, T. Gedeon, K. W. Wong, and M. Lee, Eds. Cham: Springer International Pub, 2019, pp. 150–161. doi: 10.1007/978-3-030-36711-4_14.

'''

if not os.path.exists("../data/splits/Upsampled/"):
  os.makedirs('../data/splits/Upsampled')

  ids = np.load('../data/splits/raw/all_shuffled_train_filenames.npy')
  lables = np.load('../data/splits/raw/all_shuffled_train_labels.npy')

  # as there are no lables for the test dataset, the train dataset will be used for creating the testing dataset
  x_train, x_test, y_train, y_test = train_test_split(np.array(ids), lables, test_size=0.1, random_state=False, stratify=lables)
  x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=False, stratify=y_train)

  df = pd.DataFrame(x_train,columns=['id'])
  df['lables'] = pd.DataFrame(np.argmax(y_train,axis=1))
  F1, F2, F3, F4, F5, F6, F7, F8, F9 = [x for _, x in df.groupby(df['lables'])]
  F4 = resample(F4, replace=True, n_samples=500, random_state=42)
  F5 = resample(F5, replace=True, n_samples=500, random_state=42)
  F7 = resample(F7, replace=True, n_samples=500, random_state=42)
  df = pd.concat([F1, F2, F3, F4, F5, F6, F7, F8, F9])
  x_train = df['id'].to_numpy()
  y_train = df['lables'].to_numpy()
  y_train = to_categorical(y_train, num_classes=9)

  np.save('../data/splits/Upsampled/x_train.npy', x_train)  
  np.save('../data/splits/Upsampled/y_train.npy', y_train)  
  np.save('../data/splits/Upsampled/x_test.npy', x_test)  
  np.save('../data/splits/Upsampled/y_test.npy', y_test)  
  np.save('../data/splits/Upsampled/x_val.npy', x_val)  
  np.save('../data/splits/Upsampled/y_val.npy', y_val)
  print('Upsampled Dataset split created')
else: 
  print('Upsampled Dataset split already exists')