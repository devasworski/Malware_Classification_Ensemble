# Data Processing
This folder contains all scripts required to download, preprocess and split the dataset.
The results are stored in the `data` folder. The entire dataset will require ~460GB of space on your hard drive.

### Instructions
#### 0. Install the dependencies
run the following command within this folder:
            
    pip install -r requirements.txt

This will install all dependencies required for the scripts within this folder. If you use a python environment, activate it before executing this command.
#### 1. Downlaoding the dataset
The dataset is taken from the Microsoft BIG 2015 Kaggle competition. In order to download the dataset, one needs to join the Kaggle competition ([found here](https://www.kaggle.com/c/malware-classification)). For this one needs a Kaggle account. After having joined the competition, you need to get your Kaggle username and Kaggle Key from your account section under API key.

Before downloading the dataset, it should be made sure, that the folder `/data/BIG_2015` does not exist. If it does, it should be deleted before attempting a new download. As the dataset is downloaded as a compressed file and then decompressed, if the process is interrupted, the folder should be deleted before restarting the process, as there is no other way of ensuring data integrity.

Once you have made sure the folder does not exist, you have joined the Kaggle competition and you have your Kaggle username and API key, you can execute the `download_dataset.py` script. It will prompt you to enter your Kaggle username and API key and then download the dataset automatically and decompress it.
#### 2. Preprocessing the dataset
To start the preprocessing of the dataset, the python script `preprocess_data.py` has to be executed. The process will take about 3 hours. Should the process be interrupted or preprocessed files get lost, the script will continue where it stopped and only process the missing files. Already existing files will never be overwritten.
#### 3. Create a new data splits
<p style="color:red; font-weight:bold">THIS IS NOT REQUIRED</p>
The GitHub repository contains the data splits that have been used to obtain the results presented.
Due to the nature of the ensemble model, cross-validation is not possible, nor is using a split that differs from the split used to train the individual model. <p style="font-weight:bold">If the split is changed, the entire model needs to be re-trained.</p>

To create new splits, the previous splits need to be deleted. For this, the folder `data/splits` should be delted. Afterwards, the python script `split_data.py` can be executed. This will create a new split with random seeds. 
